---
layout: post
title: A brief rant
date: 2018-07-07
description: A brief rant about new tech
img: rant_about_tech.jpg # Add image post (optional)
tags: [newstuff] # add tag
toc: true
---

I was listening to coder radio this week, and they were reviewing the top 7 reasons why engineers leave companies ([techrepublic.com](https://www.techrepublic.com/article/why-engineers-leave-your-company-the-7-most-cited-reasons/)):

* The position lacked opportunities for growth and development (49%)
* Leadership/management were bad (47%)
* The company used inefficient or obsolete technologies (31%)
* I was offered higher compensation elsewhere (21%)
* I wanted more freedom to work remotely (19%)
* There were too many stagnant projects (18%)
* I was offered better benefits elsewhere (16%)

So I understand all those reasons. I've been working in the IT industry for 14 years now, and I could rank how each one of those reasons applied to me in each of my jobs.

The thing that got me thinking was the one about 'inefficient or obsolete' technologies. While I completely 100% see why this is a problem, in my current role I am finding more and more that I'm on the opposite side of that argument. In fact I find myself being pulled in both directions.

If I had to list out the tech that we use, it would be a very long list. In the last 2 years, we've added kafka, postgres, kong and a couple of others to the 'critical path' of our application i.e. if they fall over at 3am, we get penalised and someone gets a call.

Often folks walk up to my desk and ask 'why can't we use X technology'. My answer is usually curt because generally I've had to stop what I'm doing and had to pry my attention away in order to answer this question. 'Who's going to look after it in production'? is generally what's barked back.

But for the sake of the next poor shmo walking up to my desk with another new shiny technology on the tip of their tongues, let's unpack that statement: Who's going to look after it in production? And let's pick a case study: Redis.

'Ray, why can't we use Redis a big distributed cache?' It's a great idea, and has the potential to be elegant. It addresses a problem that plagues many of our 40+ microservices in that every service author has approached caching in a slightly different way, sometimes with catastrophic consequences. In fact, most 'implementations' that I see are more like memory leaks because the cache contents are never expired, they only grow. So the potential for some level of standardisation, capacity growth enabled by horizontal scaling, reduction in memory footprint at the service level - all massive. All good.

There's a but coming - a big one.

But: assuming we get past POC stage, we're going to have to look at how it runs in on a machine other than the developer's PC, then in some sort of user facing environment, and ultimately on Production. You, mythical Redis proponent, will now get defensive and upset with me now because I'm saying 'Production' and your beautiful new idea is only the embryonic stage. 'It's still far away from production!' you'll say.

That's all very well and good, but POC's have a habit of working themselves into the product, and then at some point, another person will interrupt me my desk with an urgent request to get Redis into production because some customer facing delivery desperately needs to happen RIGHT NOW and Redis is a dependency. I'll ask: 'When did that happen? Who let that happen?' and the answer from on high will be ['JFDI'](https://www.urbandictionary.com/define.php?term=JFDI).

I've watched that movie before. It ends badly. In fact it's how most new technology lands up in the wild, and it's the root cause of a lot of our issues. So the time to ask difficult questions is *now*, while the idea is not well formed:

* What resources do you need? Redis is pretty memory heavy. This is problematic because most of our application is memory heavy. Do we have the machines/hypervisors for this? Do we need to buy more? I understand that you haven't done a lot of profiling and performance testing, but you're going to have a ballpark: we know from other technologies that do sharding/replication that we want at least 3 copies of all the data (one for maintenance, one for an accident), and we also know that anything that has a replicated data store probably wants at least 3 nodes (or an odd number) to avoid split brains. So how many: 3 or 5 or 7? If you haven't at least had a passing thought about this by now, then I'd be worried.

* You also have an idea of WHAT you're going to put into this caching technology, which should give you a ball park for how much RAM + Storage you need. If you're caching every record from every single customer, you can take a stab at how many records you'll be caching, and also what the projected growth over the next couple of years will be.

* Are you spooling to disk? Is the regular NAS storage good enough? Will there be enough?

* Do you need to run backups? Maybe as a cache, you don't need to back it up, but potentially you need to rebuild it's contents somehow? Is that slow? Does it happen organically? Will things break if the cache isn't populated?

* What do we do in a remote DC? If all the important content of this cache is in memory, what happens when we invoke our DR strategy?

* How do we install it in production? I'm not really interested in how you did it for your POC. In production we'll use ansible or some other mechanism that makes the process repeatable and reliable. Where does the package come from? Is it he OS package repo? Is there third party repo? Is it something else entirely? If you choose to use the OS repo, is that going to move fast enough i.e. are you happy to move at the same rate as the OS vendor? Do we need to apply some brakes? Same for a third party - who are they and how much do you trust them?

* How does the second installation work? A minor upgrade or even a restart. Can you do it in a rolling fashion or does this thing need downtime to make even small changes. How tolerant is the cluster to version differences?

* How do you operate this thing? How do you start it? How do you stop it? Does it need hours of rebalancing after a restart? Are there any manual interactions that have to happen when a back online after being offline?

* Is there a GUI? How are you going to maintain it? How are you going to hand off the maintenance tasks to the production operations team?

* How do you monitor this thing? And not just 'is it up', although that's an important starting point.
  * How do you tell if it's up and working? Does it support the idea of 'health'?
  * Are there any special things to look for when considering the health of the whole cluster (e.g. assert only one controller in a cluster of 7)
  * Is it running slowly? Is it about to break? Do we understand the product well enough to predict when those two things will happen? Using a java application as an example - is it garbage collecting frequently? Are the GC's taking a long time ( longer than 200ms? ). How much head room do you have in your heap? What are your threads looking like? Do you have spare capacity? Same for database connections?

* Is there any commercial support for Redis? Can we shortcut some of these questions by bringing a someone to do training?

Now, this is the thing that REALY irritates me: I actually like new technology. I think it's great. If you're not moving forward in the tech game, then you're moving backwards. You have to work to stand still. The thing that kills me is that I never get play with this stuff while it's not stressful. I'm not the one doing a spike on my machine for months. I get handed stress when the developer or whoever it is has finished 'playing', and it's time to act like a grown up. For the record, I hate being a grown up!

{% include figure image_path="/assets/img/puppy_is_for_life.jpg" alt="Don't let it end up in a pound." class="image-medium image-right" caption="A new technology, like a pet, is for life." %}

This all goes a way when the person proposing the new tech has researched it and has answers for all the difficult stuff mentioned above. Almost immediately, their very first task is to find a set of volunteers to form a Redis Technology Specialist group, and a Redis Support group. I will generally help because I am sucker and new tech excites me. Also, some of this stuff will turn out to be my actual responsibility because I look after the implementation of the monitoring and and installation.

A new technology is for life, not just for christmas. It's a gift that keeps on giving for many years to come, and you have to be ok with that.
